---
title: "More fun with command line Linux"
author: "Sean, Bronson, Debbie, and Marc"
date: "2/17/2016"
output: html_document
---

# Homework Review

***

# Tips and Tricks for working in Linux

### What is tab-completion?

During your homework, you probably noticed a few things things:

1. Linux involves a lot of typing
2. It is annoying if you misspell a word
3. You often have to know a bit about what is available so you can know what to start typing.

A really handy feature to help with this is *tab completion*. In Linux if you start to type something, even it you only know the first letter, you can hit the tab key for suggestions about what should come next. One hit on the tab key will complete if you have typed enough characters to narrow things down to only one solution, but two quick tabs will actually list out all remaining possibilities for you (and prompt you first if it's a large amount). This works to tab-complete commands as well as paths and filenames. Go ahead and try it! 


### Up and down arrows, 'CTRL-a' and 'CTRL-e'

In unix, you can always get back to previous commands by hitting the up arrow.  If you step back too far, you can page the other way by hitting the down arrow.  Find yourself at the end of a very long command and want to get back to the beginning?  Hit 'CTRL-a'.  Want to get back to the end: 'CTRL-e'.

### Copy and paste

In unix, you usually can't use the usual keystrokes for copy and paste (CTRL-C, CTRL-V). In unix, if you hightlight something with your mouse, it is automatically copied onto your clipboard. If you have one, clicking with the middle mouse button will paste. You can also try right clicking and selecting paste from the drop-down menu.

### Case sensitivity.

In Linux, the case of a letter matters. 'A' is not the same thing as 'a' etc.  So watch out for changes in case.  You will notice that some people prefer to name things using alternating lowercase and uppercase letters like this: 

```{bash, eval=FALSE}
myFile.txt
``` 
That style is called camel case.  

Other people prefer to use underscores like this:

```{bash, eval=FALSE}
my_file.txt
```

And still other people prefer all lowercase like this:

```{bash, eval=FALSE}
myfile.txt
```

Whatever style you prefer, these all have one thing in common: no spaces.  You shouldn't use white space in filenames when using linux.  You technically can do it, but everyone agrees that it's not worth it since you have to escape all your white space characters:

```{bash, eval=FALSE}
my\ file.txt
```

And this is a huge inconvenience.  So if you give someone a file like this in linux you should expect that they will probably be unhappy with you.

### What are wildcards?

In linux you can use `*` as a wildcard to indicate more broadly which things you mean.  So for example, if I have two files that both end with `.txt` then I can copy them both into myDir by doing this:

```{bash, eval=FALSE}
cp *.txt /myDir
```

Wildcards are useful for a whole host of other circumstances.  But be careful.  If you use them with dangerous commands like `rm`, you might end up wishing that you could rewind time.

### Reserved Characters (also called "Special")

Like spaces, reserved characters should be avoided in file names.  Linux has two which are prohibited.  The null character, `\0` and `/` which is the path separator.  Other reserved characters are allowed but special meaning to the system like the `-` which proceeds flags on the command line or `*` which is a wild card.  Like a space, you have to escape them which adds complexity and can lead to errors if you forget. 

***
# Basics of working with a file

### How to copy, move, rename or delete a file: Using __`cp`__, __`mv`__ and __`rm`__

As you discovered in your homework, `cp` copies a file to another place.  `mv` moves a file (and so does not leave the original behind).  Here is an example of each of these, and by now you should be familiar with the syntax:

```{bash, eval=FALSE}
cp file.txt filecopy.txt
mv file.txt ..
```

The first of these commands copies `file.txt` to another file with a different name `filecopy.txt` (in this example, the new file it would be in the same directory).  The second command moves file.txt up a directory from where it is currently located.

What if you want to rename a file? It turns out that linux does not have a concept of renaming, which may seem odd. But you can still do it. The trick is to use `mv`. But instead of moving it to a different location and keeping the same name, this time you move it to the same location but use a different name. For example:
```{bash, eval=FALSE}
mv file.txt newname.txt
```

But sometimes you may just want to get rid of something altogether.  `rm` will do that for you.  Here is an example that just removed file.txt:

```{bash, eval=FALSE}
rm file.txt
```

Another common case with rm is when you have a directory and you want to remove it AND everything in it. Remember that compliated file structure we created last time inside `~/sandbox/ourData`? If you wanted to remove `ourData ` and everything in it, you can do it recursively like this:

```{bash, eval=FALSE}
## First use ls with the -R flag to recursively show everything in the sandbox directory
cd ~/sandbox
ls -R

## now recursively remove everything in the ourData subdirectory
rm -r ourData
ls -R
```

Obviously using the -r flag with rm can be dangerous. So please be very careful!

### How to avoid creating many reduncant copies of important files: Using __`ln`__

Sometimes you will want to just create a shortcut to a file rather than actually making a copy. Perhaps it is a reference FASTA file that is very big, so making mulitple copies of it isn't economical. In the following example, notice how big the `genome.fa` file is:
```{bash}
ls -lh /tools/references/Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/
```

Instead of physically copying the file, you can just create a *soft link*. Soft links are equivalent to shortcuts in a Windows file system. A soft link is just a place holder that points to a file that actually lives somewhere else. Use the following syntax to create a soft link called myGenome.fa for this file in `~/sandbox/myData`
```{bash, eval=FALSE}
cd ~/sandbox/myData
ln -s /tools/references/Homo_sapiens/UCSC/hg19/Sequence/WholeGenomeFasta/genome.fa myGenome.fa
ls -lh
```

Notice the notation that indicates that myGenome is a link rather than a file. Also note the size: 76 bytes! Much better than 3 GB.

(Note, if there is a soft link, that must mean that there is also a hard link. There is, and it is slightly different, but that is beyond the scope of this class. Follow this syntax and stick with soft links. Trust us.) 


### How do I look at the contents of a file: __`less`__ is __`more`__

Sometimes less is more.  And when you are using linux suddenly `less` literally is a more advanced version of the `more` command.  The `more` command was long used for looking at very large text files.  And `less` does the same basic job as `more`.  Have a gigantic file that you can't even look at in excel?  `less` will probably open that file without even flinching.  Unlike text editors in windows and macs, `less` does not wait for the entire file to load into memory before showing you it's contents. Rather, it starts *paging* the file contents onto the screen so you can begin looking at it immediately. While you are doing that, it is quietly contintuing to load the rest of the file into memory in case you want to see more of it. By now you should already be somewhat familiar with how `less` works, but we will repeat the basic syntax and keystrokes that you use with `less`: 

```{bash, eval=FALSE}
cd ~/sandbox/myData
less myGenome.fa
```


Key stroke | Function |\| | Key stroke | Function |
-----------|----------|-|------------|----------|
q|Quit|\||Up or Down Arrow |Move up or down a line|
`space`|Next page|\||`/`abc|Search for text "abc"|
b|Back a page|\||n|Find next occurence of "abc"|
\#\# g|Go to line \#\#|\||?|Find previous occurence of "abc"|
G|Go to end|\||h|Show help for `less`|

Note that `less` is great for looking at your files, but you should remember that it is read-only, meaning you can't change anything. If you need to edit the contents of a file, you will need to use a text editor.

### How do I edit the contents of a file: text editors

If you need to make edits to a file, Linux is loaded with powerful text editors. Some of them (like vi or emacs) are so powerful that they could each have their own course.  Today lets look at a very simple and common one: `nano`.

```{bash, eval=FALSE}
nano ~/sandbox/myData/FEC00001_1.seq
```

That command will launch the nano editor.  Once inside you will be able to type in text of interest.  When you want to leave you can look at the bottom of the screen and you will see commands abbreviated like this: ^X 

`^X` basically means 'CTRL-X'.  It's a hint about how to give that command.  Do it now to exit out of nano and feel free to save your work on the way.

If you have significant edits to make, most of you will want to use a proper graphical text editor such as Notetab++. MobaXTerm also has a decent text editor built into it as well.

***
####  <span style="color:blue">__Exercise L1:__</span>

1. Navigate to your home directory.
2. Type `pw` and then hit the tab key twice for suggestions about what we might do next.
3. Type `ls /` and then hit the tab key twice for suggestions about directories can go to.
4. Type `ls /t ` and then hit the tab key once to complete the path. Continue downward into that directory by double- and single- tab clicks.
5. Inside the following directory `/tools/sampledata/course2` there are a series of .seq files and .pdb files. Using tab completion and wildcards, copy all the .seq files and all the .pdb into `~/sandbox/myData`. (*Hint, you will want to use two cp commands to first get the .seq files, and then the .pdb files.)


***

# More ways to interact with your files
 
### Using __`head`__ and __`tail`__ to see both sides of things

Another thing that happens with large files is that you may only want to see (or process) the first few lines.  The `head` and `tail` commands will allow you to look at the first or last few lines of any given file.
```{bash, eval=FALSE}
cd ~/sandbox/myData
head structure_1ema.pdb
```

### Finding in-file matches with __`grep`__

The grep command is usually used to find strings in text files. For example you could search your .pdb files for the string `ATOM` to show you all of the lines that contain atomic coordinates like this:

```{bash, eval=FALSE}
grep 'ATOM' structure_1ema.pdb
```
```{bash, echo=FALSE}
cd ~/sandbox/myData
grep 'ATOM' structure_1ema.pdb |head
```

This is useful all by itself, and grep is extremely powerful since it is a fast and efficient way to search your files for lines that match.  


### Merging and streaming file contents using __`cat`__ and __`zcat`__

`cat` is useful for streaming the contents of a file. Another command you will sometimes see is `zcat`, which allows you to call cat on files that are zipped (end with a `.gz` extension). When called with a single file as an argument, `cat` will simply dump the file contents onto the screen.  
```{bash}
cd ~/sandbox/myData
cat FEC00001_1.seq 
```
So by itself `cat` is not so useful.  But in combination with other commands such as `pipe`, it can be a very useful way to stream the contents of a file line-by-line to be processed by another command (more on this in a moment). 

`cat` is short for "concatenate", which means to merge together. Sometimes you would like to merge the contents of two files together, for instance to make a multi-sequence FASTA file. For this use `cat` with two files as arguments, and the contents of the two files will be displayed in succession:
```{bash}
cd ~/sandbox/myData
cat FEC00001_1.seq FEC00002_1.seq
```

Again, displaying it on the screen is not so useful. What you would really like to do is save the output to a new file. For this, you use `>`, the redirect operator.

### Redirecting output to a file: Using __`>`__ and __`>>`__

If at any time you want to save the output from a command or set of commands, you would use the re-direct operator", or "bent pipe" or `>`

```{bash, eval=FALSE}
cd ~/sandbox/myData
cat FEC00001_1.seq FEC00002_1.seq > FEC_seq.fasta
less FEC_seq.fasta
```

If the file does not exist, it will be created for you. If it does exist, it will be overwritten. Sometimes though, you might want to simply append to an existing file without overwriting it.  In that case you would use the "double bent pipe" or `>>`.  So now we can add additional sequences to our fasta file:

```{bash, eval=FALSE}
cat FEC00003_1.seq >> FEC_seq.fasta
less FEC_seq.fasta
```

### Redirecting output into another command: __`pipes`__ or __`|`__

You have seen how you can use `>` or `>>` to redirect output to a file. Using the __pipe__ redirect or `|` it is also possible to redirect the output of a command directly into another. This is extremely useful because it allows you to use commands in combination to do more interesting types of things. Here are some examples:

We have already seen that you can use `grep` to search for a string of text line by line. 
```{bash, eval=FALSE}
cd ~/sandbox/myData
grep ATOM structure_1ema.pdb
```
The problem with this is that it dumps a large amount of data directly onto your screen. You could minimize that by piping the output to `less` where you can now browse through the output more easily. Notice that we didn't have to save it to a file first (which gets to be awkward). Similarly, we could also just look at the first few lines of the output by piping to `head`.
```{bash, eval=FALSE}
grep 'ATOM' structure_1ema.pdb | less
grep 'ATOM' structure_1ema.pdb | head
```

We can also do a more sophisticated grep search by piping the output of our grep search into another grep. For instance, what if we wanted to look at all the atomic coordinates for just proline residues? We can do successive grep searches on the terms 'ATOM' and 'PRO' and look at the results in `less`:
```{bash, eval=FALSE}
grep 'ATOM' structure_1ema.pdb | grep 'PRO' | less
```
Notice that the file name for 'structure_1ema.pdb' is only ever specified in the first command string. We don't need to specify it in the second `grep` call, because it is receiving it's input from the pipe. Similarly, we don't need to specify a source for `less` as it receives it's input from the second pipe. 

This will be a very important feature as you 'pipe' different commands together to create powerful custom commands.


### Counting things using __`wc`__

The `wc` command is the word count command.  Check out the man page for usage. This command is sort of a curiosity on it's own, but in combination with other commands it can become very useful.  For example, you can use the `-l` flag to specify line count. So if you wanted to know how many things were in the current directory you could do this:

```{bash}
ls ~/sandbox/myData | wc -l
```

If you wanted to know how many sequences are in a fastq file, you can take advantage of the fact that sequence names are identified with the `>` character. So:
```{bash, echo=FALSE}
cd ~/sandbox/myData/
cat FEC00001_1.seq FEC00002_1.seq FEC00003_1.seq> FEC_seq.fasta
```

```{bash}
cd ~/sandbox/myData/
grep '>' FEC_seq.fasta | wc -l
```

### Parsing your files: Using __`cut`__,  __`sort`__, and __`uniq`__

A surprisingly large portion of bioinformatics work is that of parsing your data files, or in other words, extracting and organizing the useful bits of information so that you can ask the questions you want. The linux command line has a multitude of tools that can parse data files. However...

These tools are notoriously picky. The command syntax is very unforgiving, and the format of your data files can give you headaches if they aren't organized just so. So these commands are useful for small tasks where you want to find a simple answer, do a quick check, etc. But if you have more serious data munging tasks (and most of you do) there are much easier ways to do this. High level scripting languages such as R, python, or perl are almost always infinetly easier to use and more powerful. That is why workflows almost invariably flow from linux command line into one of these other scripting languages and why we will focus more on learning these tasks in R than from the shell.  

Here are some simple examples of how you could parse a file. Consider this following tab delimited data file, Marrus_claudanielis.txt: This file contains information about a series of deep sea expeditions, including the name of the dive, the date, lattitude, longitude and depth. 
```{bash}
cd ~/sandbox/myData/
cat Marrus_claudanielis.txt
```

Use `cut` to display only specified columns: 
```{bash, eval=FALSE} 
cut -f1,2,5 Marrus_claudanielis.txt
```

If you want to put the output in a particular order, you can use the `sort` command. 
```{bash, eval=FALSE}
cut -f1,2,5 Marrus_claudanielis.txt | sort 
```

Remove duplicate rows. Usually you will have to sort first. Depending on what you are looking for, you may also have to isolate a particular column or even a substring inside that column using `cut`. You may also want to get rid of the column names using `tail` to avoid confusion. What are the unique dive names? 

```{bash, eval=FALSE}
tail -n +2 Marrus_claudanielis.txt | cut -d " " -f1 | sort | uniq
```

Use the man pages for `tail`, `cut`, `sort`, and `uniq` to learn about each of the flags used in this example and understand the logic of how this command was constructed. As you can see though, this starts to get a little out of hand, and when we get into R, you will find that there are better tools for parsing down to this level. 


### Compression and decompression with __`tar`__

`tar` is a useful compression and decompression utility.  Sometimes you will see files that end with a `.tar` extensions.  These are just files that have been compressed with the `tar` utility.  Usually, you will see files that end with the double extension `.tar.gz`.  These are are actually double compressed files that have been compressed with both `tar` and `gzip` (zip) compression. Files that look like this are often called 'tarballs'. The good news is you can use tar to unpack those with ease.  Here is an example of how you can unpack a tarball using tar: 

```{bash, eval=FALSE}
tar -zxvf myFile.tar.gz
```



### Fixing bad line endings with __`dos2unix`__

`dos2unix` is a useful command to know about. Text files from windows will typically have different line endings from text files from unix. And that can be a problem if you need to parse files by line. `dos2unix` fixes files that have been formatted for windows so that the line endings are the same.  So the following command will fix the `myFile.txt`.

```{bash, eval=FALSE}
dos2unix myFile.txt
```

If you are wondering whether your file has the wrong kind of line endings, you can see them by either looking at the file using the emacs text editor OR by feeding them through cat and using the -v flag like this: 

```{bash, eval=FALSE}
cat -v myFile
```

If when you do the above command, a file with windows style line endings will have ^M on the end of all the lines.  

***
####  <span style="color:blue">__Exercise L2:__</span>

1. Navigate to your home directory.
2. Using cat, concatentate FEC00007_1.seq and FEC00007_2.seq and save the output to a file called FEC00007.seq.
3. Using cat, append FEC00007_3.seq to FEC0007.seq.
4. Using nano, open FEC0007.seq and edit the sequence names.
4. Using ls, grep, wc, and pipe, display all files in ~/sandbox/myData that have the `.seq` extension and output a count of these to the screen.

***
***
#Break

***
***
# Additional tools to know about

### Using __`find`__

The find command is for finding files in the filesystem.  But it is actually VERY powerful because like everything else today it can be paired with the other toys in the Linux toybox.  The find command takes a couple of arguments. The 1st is the part of the directory to search in and the secons is what to look for.  A common way to use find is to find a file with a specific name.  So to find a file named `bob.txt` in your local directory could could do this

```{bash, eval=FALSE}
find . -name 'bob.txt'
```

But as I stated before find is quite powerful.  You can also do partial matches, and choose from a huge array of other flags etc. to find things depending on what you happen to remember.


### Using __`who`__ and __`top`__

Sometimes you want to know who else is logged in. IOW, if there are multiple machines to work on, you might prefer to work on a less crowded one...

Other times when you log into a computer you might be wondering what it is doing overall.  This is what the top command is for.  If you call top, you will get a high level overview of what the computer is doing right now.  Top will tell you what processes are running, and how many resources they are using.  And by using it you can spot problems.  For example, you might want to know about long running processes that are consuming up all the system resources and slowing you down.


### Using __`ps`__ and __`kill`__

Top is useful for seeing whether there is a problem, but it updates frequently and only tends to display long running processes that are using a lot of resources.  Sometimes you just want to see all the things that you are running.  `ps` is a good way to list all the processes that you have running.  This command will list all the processed run by a particular user:

```{bash, eval=FALSE}
ps -U username
```

Once you have done that, you will see an output that looks like this:


```{bash, eval=FALSE}
[username@EWRLNXRD29 SCRICompBioCourses]$ ps -U username
  PID TTY          TIME CMD
 5101 ?        00:00:00 sshd
 5106 ?        00:00:00 sshd
 5108 ?        00:00:00 sftp-server
```

The `PID` stands for the process identifier.  It's a unique number that can be used to specify precisely which process you mean.  This is important, because if you make a mistake and accidentally launch some long running resource hogging process, you might not want to wait (or make others wait) for it to finish.  In that case you might need to `kill` it.  Here is an example of how you could do just that:


```{bash, eval=FALSE}
kill 5106
```

The above command will kill the process where the PID = 5106.


### Using __`bg`__ and the __`&`__ operator

Sometimes you have a job that you want to run and you don't want to wait for it to finish before you get your terminal back.  When that happens you can hit [CTRL-Z] to get your command prompt back and then use the `bg` command to continue that process in the background.

```{bash, eval=FALSE}
commandThatTakesALongTimeToRun
[CTRL-Z]
bg
```

Or if you know something will run for a while, you can plan ahead and use the `&` operator.
```{bash, eval=FALSE} 
commandThatTakesALongTimeToRun &
```


### Using __`nohup`__ 

Sometimes you will want a job to run and you already know that it's going to take several hours. When that happens, you might worry that if you log out your job will not complete. For this circumstance you will want to use the `nohup` command (or: no hang up).  You can use it to run a command in the background.  `nohup` will keep running that process untill it either finishes or until someone kills it.  You can use `nohup` like this:

```{bash, eval=FALSE}
nohup command &
```


***
####  <span style="color:blue">__Exercise L3:__</span>

1.  Use `find` to search for the file `Marrus_claudanielis.txt` in the folder `~/sandbox`.
2.  Use `find` to search for the file `Marrus_claudanielis.txt` in the folder `/`.

    + This should take a while. Sent the process to the background by entering `CTRL-Z` followed by `bg`
    + Discover the process id by entering `ps -U username`, substituting in your userid. Look for the PID associated with the `find` command.
    + Use `kill` to end the process by entering `kill PID`, substituting in the process id for your search.

***

# The Linux file structure

### Root

### Common system directories

You may have noticed that there are many directories contained in `root`, and you may wonder what these are for and if they are important to you. Most of these are system directories that are common to nearly every Linux OS. __They have specific functions, and unless you really know what they are for and are authorized to change them, it is best to leave these alone.__ To satisfy your curiosity, here is a brief description of some of the common ones:

* __`/bin`__: This directory contains many *binary* files for the common commands such as `ls`, `mv`, `cp`, etc.

* __`/dev`__: This directory contains system files related to any attached *devices*, such as hard drives, DVD drives, modems, speakers, etc.

* __`/etc`__: This stands for *et cetera*. But that doesn't mean that the contents are miscellaneous, although it's likely it once meant that. Now, this drive contains all system related configuration files. It is considered the nerve center of the system.

* __`/home`__: Contains each user's home directories.

* __`/lib`__: This directory contains the system *library*. Libraries are modules of code that provide key functionality. Think of them as mini code repositories.

* __`/usr`__: This directory often contains the largest share of data. `usr` stands for *User System Resources* (not 'User'). This directory contains many more binaries, libraries and configuration files for all the additional, non-system applications that have been installed on the computer.

* __`/tmp`__: This directory contains mostly files that are only required *temporarily*. Just because they are temporary though doesn't mean they aren't important. Many of these files are important for currently running programs and deleting them may result in a system crash.


### SCRI-specific directories

There are two directories in `root` that are specific to the SCRI environment and that will be key to successfully working with the autobots:

* __`/tools`__ The tools directory contains a collection of bioinformatics related applications, programs and tools that we have pre-compiled for your use. I want to briefly point out a few interesting locations:

    + __`/tools/BioBuilds-xxxx.xx/`__: Most of the bioinformatics tools that you will need have been obtained through a software bundle called __`BioBuilds`__. Biobuilds gets updated about twice a year. As it gets updated, we add a new time-stamped directory containing the updated bundle of tools. By default our system will point to the latest version of the tools, but older ones are left there in case you still need them. If you want, you can look inside and see what is available. In practice though, you should never really need to physically look around in here. If you want to know if a particular program is available and ready to execute, such as the BWA aligner, the easiest thing to do is to use the `which` command. 
```{bash, eval=FALSE}
which bwa
```
The output of this will either tell you the path of where this command lives or will tell you that it couldn't find it in any of the expected locations (meaning it either isn't installed or it isn't ready to be executed easily).


    + __`/tools/BioBuilds-2014.04/share/java`__: One of the more useful utilities included in BioBuilds is one called `picard`. `picard` is a collection of mini programs that allow you to manipulate sequence files and alignment files. It is a java script `jar` file, which means that it executes a little differently than other programs. It has to be called from `java` and you will have to supply the full path to it. So in case you are wondering, here is how you call picard:
```{bash, eval=FALSE}
java -jar /tools/BioBuilds-2015.04/share/java/picard/picard.jar
```
The output of this will be a list of all the mini programs available through `picard`. We will go over how to use some of these at another time, or as needed through drop-in office hours.


    + __`/tools/references`__. This directory contains reference sequences and genome annotations for a number of model organisms. If there are additional tools or references that you think you may need frequently and that could be of interest to others generally, you can request that they be added by sending an email to ResearchScientificComputing@seattlechildrens.org.


* __`/data`__ The data directory provides some local scratch space for users. The term __"local"__ just means that that space is physically located on the computer you are using. Therefore the read/write speed to this disk is relatively fast compared to a mounted (remote) file system. The term __"scratch space"__ means that the space is not intended to keep anything permanently. It is a temporary holding spot. The data directory provides you a convenient local place to temporarily stash files that you are actively working on. When you are done with them, they should be deleted or moved elsewhere to free up this space for others. The `/data` directory provides 2 TB of shared space. __Note: You should not use your `home` directory for scratch space. Use `/data` instead.__ 

***
####  <span style="color:blue">__Exercise L4:__</span>
* Using the `which` command, discover if the tools `tophat` and `velvetg` are available.
* Navigate to `/data`. Feel free to make a new directory there with your user id. <Hint: use the `mkdir` command.>

*** 

### The best place to read and write
You may be asking "If I can't stash stuff in my home directory because it's too small and the data directory is only for scratch space, then where am I supposed to store my stuff?" The answer is that this is the intended use of your departmental share drives. 

Unlike your O drive, departmental share drives are not automatically issued. They are provisioned by request to departments, centers, or PIs, and access to a drive is controlled by permissions. It is likely that you already have access to one or more departmental shares through your lab. Share drives are backed up to tape on a daily basis, so the information stored there is very secure and easily recoverable. Also, share drives typically provide space enough for larger amounts of information, so they are really the best place to be storing your scientific data.  

These drives can be mounted (or mapped to use the Windows phrase) on the Linux system and accessed directly. __The best practice for doing work on these machines is to read from and write directly to your departmental share drives.__ If performance is an issue, you can temporarily write to `/data`, but please remember to move or delete when finished. You should not regularly write output to `/home`.

In order to get access to your share drives, we have written a helper program that will mount the share for you.

***
####  <span style="color:blue">__Exercise L5:__</span>

* Find out what departmental shares you have access to:
    + Open 'My Citrix Computer' from your desktop.
    + In the address bar, type `\\childrens\research\` and hit enter
    + You will now see a list of departmental shares that you have access to.
    
* In your Linux terminal, run the following commands
```{bash, eval=FALSE}
cd ~
Mountscript
```
Follow the prompts to add one or more of your departmental shares. Start with the one that you will primarily use to store your data. Be sure to follow the instructions regarding the use of forward slashes `/` for path separators.

* If you have been successful, you will now see a series of new directories in your home drive: `share0`, `share1`, etc depending on how many drives you mounted. You will also see a new text file called `share_list`. This file simply records the names of the departmental shares that you have tried to mount. Try navigating through the file structure of your newly mounted shares
```{bash, eval=FALSE}
cd share0
ls -l
```

* When you are done with your Linux session, it is good practice to unmount your drives before you log out. We are working on a way to do this automatically. In the meantime, you will want to manually unmount each drive as follows:
```{bash, eval=FALSE}
cd ~
unmount share0
```
When you do this, you will see that `share0` is still there, but now it's contents are empty.

*** 
# End of Class

***
***


### Cheat sheets
We have included some cheat sheets that could come in handy for you down the road. These were taken from a book called *Practical Computing for Biologists* by Steven Haddock and Casey Dunn. It is an excellent reference and highly recommended for anyone who would like to go deeper than what we have had time to do.

#### Appendix 2: Regular Expressions
Regular expressions are one of the most powerful tools for command line users. A startling number of problems you will encounter can be reformulated as some kind of simple text manipulation. These manipulations will follow patterns that easy for humans to define, but difficult for simple search and replace engines like you find in Excel or Word. For example, you may have a list of species as follows
```{bash, eval=FALSE}
Homo sapiens
Drosophila melanogaster
Caenorhabditis elegans
Danio rerio
```
You would like to easily rename these to the more common short form
```{bash, eval=FALSE}
H. sapiens
D. melanogaster
C. elegans
D. rerio
```
The pattern is easy enough: abbreviate the first word to the first letter, add a period, and retain the second word. But can you accomplish this with a simple search and replace in Excel? You would have to individually search for the strings *Homo*, then *Drosophila*, etc and replace each one, which is not very efficient.

Regular expressions provide a powerful way to perform these sorts of pattern based search and replace queries. The above problem would be mind-blowingly trivial for a single regular expression query, even if the list were hundreds of species long. Using regular expressions could fill a course on its own. We will touch only briefly on these in Course 4, but we encourage you to explore them more on your own or come to office hours where we can spend more time on them.

#### Appendix 3: Shell Commands
A list of many of the most common shell commands. For most of you, these will cover ~95% of all of your Linux command needs, so they are worth spending time getting to know.


***
# Homework 


####  <span style="color:blue">__Homework 1: Upack a archive file (.tar.gz)__</span>
When getting data from your data center or from a repository, it is often compressed into an archive file. It is important to know how to work with these.
1. Copy the compressed archive file `/tools/sampledata/course2/homework.tar.gz` into `~/sandbox/myData`
2. Discover what is in the archive file using the `tar` command. Using the `man` page, set the appropriate flags to use an archive file and verbosely list the contents. 
3. Unpack this file using the `tar` command. Usiong the `man` page, set the appropriate flags to use an archive file, filter the archive file through gzip, extract the files from the archive, and verbosely list the files processed.

####  <span style="color:blue">__Homework 2: __</span>
If you generate a text file on your Windows machine and then try to work with it on linux, you may run into issues with the fact that windows style line endings are not recognized properly in unix systems. It is therefore good to know how to check and correct those line endings.
1. Find the file 'badtextfile.txt' in ~/sandbox/myData. Use the `find` command if you have to.
2. Using `cat -v` check the line endings in this file.
3. Convert the line endings from dos to unix style line endings. Recheck the line endings to ensure it worked.
4. Rename the file 'goodtextfile.txt'

####  <span style="color:blue">__Homework 3: __</span>
You often want to know how many entries you have in a particular data file. This can be daunting, especially if the data file is very big. 
1. Go to `~/sandbox/myData/homework/sequences`
2. Here you will discover four sequence files produced by a Illumina HiSeq instrument in fastq format. A fastq file contains sequence and associated quality scores for potentially millions of sequence reads. A fastq read consists of four lines: 

* A unique sequence identifier starting with `@`

* The read sequence

* `+`

* The per base quality score (ASCII encoded)

Using `zcat`, `pipe`, and `head`, print out the first 2 *reads* of one of the fastq files. 

3. How many reads are contained in each of the fastq files? (Hint: you might want to make use of `grep`.)

####  <span style="color:blue">__Homework 4: __</span>
Creating links is a useful way to deal with large data files that you don't want to copy.
1.  The `/tools/references` directory contains reference sequence information for several model organisms. Create a link in `~/sandbox/myData` to the whole genome fasta file for the _D. melanogaster_ dm3 referene sequence. Call the link flyGenome.fasta.
2. Genome fasta files contain sequence information for all chromosomes. Each chromosome is delineated by `>Chromosome Name`. How many chromosomes are given in the dm3 genome?


####  <span style="color:blue">__Homework 5: __</span>
It is common to pass as an argument a text file that contains the paths to several input files. This homework example will give you some ideas on how to do that. 
1. Go to `~/sandbox/myData/homework/expression`
2. Here you will discover four directories corresponding to four samples in a RNASeq experiment. Inside each sample directory is a gene annotation file called `transcripts.gtf`. 
3. Use `ls` with `*` and `>` to output the paths to each of these files and save it to a new file called `assembly_GTF.txt`

####  <span style="color:blue">__Homework 6: __</span>
You may get an output file that contains many columns and rows. You can do some gentle parsing of this to get a sense of the data. If you want to do more complex parsing, it's best to use something like R or python.
1. Go to `~/sandbox/myData/homework/de`.
2. Here you will discover four files with a `.diff` extension. These are outputs from a differential expression analysis. The analysis looked for differential expression based on gene region, coding sequences, promotor regions, or alternative splicing sites.
3. Take a peak inside the file `gene_exp.diff` to see how it is structured.
4. Using `cut`, parse the file to only show the columns "gene", "locus", "status", and "p_value".
5. Using `cut` to show the same columns, try to find to the top 20 most differentially expressed genes using `sort` and `head`. These are the genes who's status is "OK", and that have the smallest p-value.

***
#__Answers for Homework Exercises__ 
####  <span style="color:blue">__Homework 1: Upack a archive file (.tar.gz)__</span>
When getting data from your data center or from a repository, it is often compressed into an archive file. It is important to know how to work with these.
1. Copy the compressed archive file `/tools/sampledata/course2/homework.tar.gz` into `~/sandbox/myData`
2. Discover what is in the archive file using the `tar` command. Using the `man` page, set the appropriate flags to use an archive file and verbosely list the contents. 
3. Unpack this file using the `tar` command. Using the `man` page, set the appropriate flags to use an archive file, filter the archive file through gzip, extract the files from the archive, and verbosely list the files processed.

```{bash, eval=FALSE}
cd ~/sandbox/myData
cp /tools/sampledata/course2/homework.tar.gz .

tar -tvf homework.tar.gz
tar -zxvf homework.tar.gz
```

####  <span style="color:blue">__Homework 2: __</span>
If you generate a text file on your Windows machine and then try to work with it on linux, you may run into issues with the fact that windows style line endings are not recognized properly in unix systems. It is therefore good to know how to check and correct those line endings.
1. Find the file 'badtextfile.txt' in ~/sandbox/myData. Use the `find` command if you have to.
2. Using `cat -v` check the line endings in this file.
3. Convert the line endings from dos to unix style line endings. Recheck the line endings to ensure it worked.
4. Rename the file 'goodtextfile.txt'

```{bash, eval=FALSE}
find ~/sandbox/myData -name badtextfile.txt
cd ~/sandbox/myData/homework

cat -v badtextfile.txt
dos2unix badtextfile.txt
mv badtextfile.txt goodtextfile.txt
```

####  <span style="color:blue">__Homework 3: __</span>
You often want to know how many entries you have in a particular data file. This can be daunting, especially if the data file is very big. 
1. Go to `~/sandbox/myData/homework/sequences`
2. Here you will discover four sequence files produced by a Illumina HiSeq instrument in fastq format. A fastq file contains sequence and associated quality scores for potentially millions of sequence reads. A fastq read consists of four lines: 

* A unique sequence identifier starting with `@`

* The read sequence

* `+`

* The per base quality score (ASCII encoded)

Using `zcat`, `pipe`, and `head`, print out the first 2 *reads* of one of the fastq files. 

3. How many reads are contained in each of the fastq files? (Hint: for this file, all sequence identifiers start with the string "@HWI")

```{bash, eval=FALSE}
cd ~/sandbox/myData/homework/sequences
zcat H_KH-540077-Normal-cDNA-1-lib1_ds_10pc_1.fastq.gz | head -n 8
zcat H_KH-540077-Normal-cDNA-1-lib1_ds_10pc_1.fastq.gz | grep "@HWI" | wc -l
```

####  <span style="color:blue">__Homework 4: __</span>
Creating links is a useful way to deal with large data files that you don't want to copy.
1.  The `/tools/references` directory contains reference sequence information for several model organisms. Create a link in `~/sandbox/myData` to the whole genome fasta file for the _D. melanogaster_ dm3 referene sequence. Call the link flyGenome.fasta.
2. Genome fasta files contain sequence information for all chromosomes. Each chromosome is delineated by `>Chromosome Name`. How many chromosomes are given in the dm3 genome?

```{bash, eval=FALSE}
cd ~/sandbox/myData
ln -s /tools/references/Drosophila_melanogaster/UCSC/dm3/Sequence/WholeGenomeFasta/genome.fa flyGenome.fasta
grep ">" flyGenome.fasta
grep ">" flyGenome.fasta | wc -l
```

####  <span style="color:blue">__Homework 5: __</span>
It is common to pass as an argument a text file that contains the paths to several input files. This homework example will give you some ideas on how to do that. 
1. Go to `~/sandbox/myData/homework/expression`
2. Here you will discover four directories corresponding to four samples in a RNASeq experiment. Inside each sample directory is a gene annotation file called `transcripts.gtf`. 
3. Use `ls` with `*` and `>` to output the paths to each of these files and save it to a new file called `assembly_GTF.txt`
```{bash, eval=FALSE}
cd ~/sandbox/myData/homework/expression
ls */transcripts.gtf > assembly_GTF.txt
```


####  <span style="color:blue">__Homework 6: __</span>
You may get an output file that contains many columns and rows. You can do some gentle parsing of this to get a sense of the data. If you want to do more complex parsing, it's best to use something like R or python.
1. Go to `~/sandbox/myData/homework/de`.
2. Here you will discover four files with a `.diff` extension. These are outputs from a differential expression analysis. The analysis looked for differential expression based on gene region, coding sequences, promotor regions, or alternative splicing sites.
3. Take a peak inside the file `gene_exp.diff` to see how it is structured.
4. Using `cut`, parse the file to only show the columns "gene", "locus", "status", and "p_value".
5. Using `cut` to show the same columns, try to find to the top 20 most differentially expressed genes using `sort` and `head`. These are the genes whose status is "OK", and that have the smallest p-value.
```{bash, eval=FALSE}
cd ~/sandbox/myData/homework/de
less gene_exp.diff

## Before doing cut, it can be helpful to print out the column headers:
head -n 1 gene_exp.diff
cut -f3,4,7,12 gene_exp.diff
cut -f3,4,7,12 gene_exp.diff | grep "OK" | sort -k 4n | head -n 10
```